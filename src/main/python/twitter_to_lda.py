#!/usr/bin/env python

#######
####### twitter_to_lda.py
#######
####### Copyright (c) 2011 Ben Wing.
#######

import sys, re
import math
from optparse import OptionParser
from nlputil import *

############################################################################
#                               Quick Start                                #
############################################################################

# This program reads in data from the user_pos_word file and related files,
# generated by preproc/extract.py from the original file "full_text.txt"
# in the Geotext corpus of Eisenstein et al. 2010, located at
#
# http://www.ark.cs.cmu.edu/GeoText/
#
# It then converts the data to "LDA" format, splitting it into train/dev/test
# files in the process.  These files are then used by
# twitter_geotext_process.py.
#
# To run:
#
# (1) The files user_info.train, user_info.dev, and user_info.test are
#     needed, to indicate what goes in which splits.  These files need to
#     be copied from the processed_data subdir of the Geotext corpus, e.g.
#
#       cp processed_data/user_info.* processed-20-docthresh
#
# (2) Normally, cd to the directory containing the files output by extract.py,
#     and execute e.g.
#
#     PATH_TO_SCRIPT/twitter_to_lda.py -i . -o .
#
#     This will generate the LDA-format files train.dat, dev.dat, and test.dat.

############################################################################
#                              Documentation                               #
############################################################################

# This program is intended to be used after extract.py (part of the Geotext
# corpus) is rerun, possibly changing settings, esp. doc_count_thresh, which
# we reduce below its original value of 40.
#
# The format of user_pos_word is a series of lines like this:
#
# 1       6       6
# 1       7       7
# 1       8       8
# 1       9       1
# 1       10      9
# 1       11      10
# 1       12      11
# 1       13      12
# 1       14      13
#
# where each line contains "document-id, position, word-id" where a
# "document" is a series of tweets from a single user.
#
# Other files needed on input are "user_info", "user_info.train",
# "user_info.dev" and "user_info.test".
#
# user_info has lines like this:
#
# USER_79321756   47.528139       -122.197916
# USER_6197f95d   40.2015 -74.806535
# USER_ce270acf   40.668643       -73.981635
#
# which identify the user and latitude/longitude of each "document".
# Note that there is no repetition in the user tags, i.e. each document
# is a given a separate "user tag" even if a particular user produced more
# than one document.  The latitude/longitude pairs do repeat, sometimes.
#
# The other three user_info.* files are in the same format but list only
# the documents going into each of the three train/dev/test sets (identiable
# by the user tag).
#
# A sample line in LDA format is as follows:
#
# 259 40.817009 -73.947467 234:2 636:1 402:3 67:4 603:1 670:2 369:1 235:1 34:7 436:4 637:1 336:1 604:1 269:3 135:4 671:1 1:85 437:1 638:1 169:1 404:1 605:1 672:1 2:4 203:2 572:1 36:7 438:2 639:2 505:1 70:1 606:1 137:2 673:1 3:3 104:2 640:1 372:1 37:14 138:1 71:3 607:2 674:1 4:1 641:1 373:1 407:1 675:1 72:2 608:1 139:2 5:2 642:1 39:4 140:1 676:1 609:2 542:3 408:1 643:1 509:1 174:2 40:1 677:1 275:1 141:1 342:1 7:1 610:2 443:1 644:3 309:2 678:1 611:1 645:5 377:1 42:1 679:1 210:1 612:1 110:1 646:1 680:1 144:2 77:2 613:2 245:3 111:1 647:1 681:1 145:1 480:1 11:3 212:3 614:1 279:1 179:1 112:1 648:1 682:1 280:1 146:1 213:1 615:3 649:1 415:1 683:1 13:61 616:1 650:1 684:1 14:1 550:2 617:1 383:1 182:2 115:6 651:1 685:1 350:1 15:6 82:3 618:1 183:1 49:1 585:2 652:2 317:1 518:2 686:1 16:17 619:1 351:1 586:1 251:1 653:3 687:1 84:2 620:1 51:6 587:1 654:2 688:1 420:1 85:4 621:3 286:3 152:1 588:7 454:1 119:9 655:1 689:1 622:1 287:1 153:1 53:1 589:1 455:1 656:1 690:1 87:8 623:1 54:1 590:1 255:2 121:1 657:1 322:4 389:1 691:1 88:1 624:1 21:12 591:1 256:1 658:1 323:1 189:1 390:1 55:1 692:1 625:2 290:3 223:1 659:1 391:4 56:5 592:1 693:1 90:7 626:1 358:3 660:1 593:1 694:1 91:5 627:1 359:3 24:9 225:3 661:1 58:21 594:1 695:1 25:1 628:1 494:1 193:1 662:1 327:2 595:1 696:1 26:4 428:1 629:1 127:21 663:1 60:1 596:1 27:2 563:2 630:1 128:4 664:1 530:1 597:4 229:1 430:1 631:3 665:1 598:1 263:3 163:2 632:3 398:4 599:1 666:1 633:1 600:1 667:1 567:2 433:1 634:1 500:1 199:2 601:1 266:3 467:3 668:3 434:3 32:9 635:1 300:2 501:1 166:1 602:1 669:1 334:1
#
# This gives first the number of word types seen in the document, followed
# by latitude and longitude, then a pair ID:COUNT for each word type, listing
# the type's ID and the count of how many times that word is seen in the
# document.

############################################################################
#                                   Code                                   #
############################################################################

# Debug level; if non-zero, output lots of extra information about how
# things are progressing.  If > 1, even more info.
debug = 0

# If true, print out warnings about strangely formatted input
show_warnings = True

#######################################################################
###                        Utility functions                        ###
#######################################################################

def uniprint(text):
  '''Print Unicode text in UTF-8, so it can be output without errors'''
  if type(text) is unicode:
    print text.encode("utf-8")
  else:
    print text

def warning(text):
  '''Output a warning, formatting into UTF-8 as necessary'''
  if show_warnings:
    uniprint("Warning: %s" % text)

#######################################################################
#                            Process files                            #
#######################################################################

vocab_id_to_token = {}
# Mapping from user tag (e.g. USER_ce270acf) to document ID (numbered
# starting at 1).
user_id_to_document = {}
# Mapping from document ID to latitude/longitude.
document_to_latitude = {}
document_to_longitude = {}
# Mapping from document ID to word-count dictionary.
document_to_word_count = {}
# Mapping from one of 'train', 'dev', or 'test' to a list of user tags.
user_id_by_split = {}

# Process combined user_info file
def read_user_info(filename):
  id = 0
  for line in open(filename):
    id += 1
    userid, lat, long = line.strip().split('\t')
    #errprint("%s: %s" % (id, userid))
    if userid in user_id_to_document:
      errprint("User %s seen twice!  Current ID=%s, former=%s" % (
        userid, id, user_id_to_document[userid]))
    user_id_to_document[userid] = id
    document_to_latitude[id] = lat
    document_to_longitude[id] = long

# Process split user_info file
def read_user_info_split(split, filename):
  user_id_by_split[split] = []
  for line in open(filename):
    userid = line.strip().split('\t')[0]
    user_id_by_split[split] += [userid]

# Read user_pos_word
def read_user_pos_word(filename):
  for line in open(filename):
    doc, pos, word = line.strip().split('\t')
    doc = int(doc)
    if doc not in document_to_word_count:
      document_to_word_count[doc] = intdict()
    document_to_word_count[doc][word] += 1

# Output file in LDA format for given split
def output_lda_file(split, filename):
  outfile = open(filename, "w")
  for userid in user_id_by_split[split]:
    docid = user_id_to_document[userid]
    words = document_to_word_count[docid]
    outfile.write("%s " % len(words))
    outfile.write("%s %s" % (document_to_latitude[docid],
      document_to_longitude[docid]))
    for word, count in words.iteritems():
      outfile.write(" %s:%s" % (word, count))
    outfile.write('\n')
  outfile.close()

#######################################################################
#                                Main code                            #
#######################################################################

def main():
  op = OptionParser(usage="%prog [options] input_dir")
  op.add_option("-i", "--input-dir", metavar="DIR",
                help="Input dir with Geotext preprocessed files.")
  op.add_option("-o", "--output-dir", metavar="DIR",
                help="""Dir to output processed files.""")
  op.add_option("-d", "--debug", metavar="LEVEL",
                help="Output debug info at given level")

  opts, args = op.parse_args()

  global debug
  if opts.debug:
    debug = int(opts.debug)
 
  if not opts.input_dir:
    op.error("Must specify input dir using -i or --input-dir")
  if not opts.output_dir:
    op.error("Must specify output dir using -i or --output-dir")

  splits = ['train', 'dev', 'test']
  read_user_info("%s/%s" % (opts.input_dir, "user_info"))
  for split in splits:
    read_user_info_split(split, "%s/%s.%s" %
      (opts.input_dir, "user_info", split))
  read_user_pos_word("%s/%s" % (opts.input_dir, "user_pos_word"))
  for split in splits:
    output_lda_file(split, "%s/%s.dat" % (opts.output_dir, split))

main()

